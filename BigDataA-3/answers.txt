Q1. What was wrong with the original wordcount-5 data set that made repartitioning worth it? Why did the program run faster after?

Ans. The data was badly partitioned, some file sizes were large for up to 200MB and some were too small, therefore repartitioning was necessary to improve the performance. The program ran faster because of the repartitioning as all the executors had almost same amount of work, they were not sitting idle after completing a task quickly either they were taking too long for the task to complete as the files were partitioned evenly.

Q2.The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower?) Why? [For once, the answer is not “the data set is too small”.]

Ans. The input file sizes in wordcount-3 are not uneven, the data is already partitioned properly, so there is no need of repartitioning the data which can add to the cost of processing. Hence the program would work slower on wordcount-3.

Q3.How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible? (It's possible to get about another minute off the running time.)

Ans. Partitioning the input data into even partitions before passing it to spark for processing can significantly improve performance.

Q4.When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the desktop/laptop where you were testing)?

Ans. Good range for me was 25-35.

Q5.How much overhead does it seem like Spark adds to a job? How much speedup did PyPy get over the usual Python implementation?

Ans. The overhead was between 10 to 15 seconds. For 10^8 I was getting 22.34 with python3 and 6.29 with PyPy, therefore PyPy was approximately 3.5 times faster.